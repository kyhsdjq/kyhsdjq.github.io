---
title: "RAG"
date: 2025-11-25 20:16:00 +0800
categories: [AI, AI Agent]
tags: [note, computer science, ai, ai agent, rag]     # TAG names should always be lowercase
author: kyhsdjq
description: RAG basic knowledge points.
math: true
mermaid: true
---

## 简介

RAG 即 Retrieval-Augmented Generation，旨在通过 **检索** 来增强 LLM 回答的准确性和可靠性。一方面，为生成的回答提供可信的信息来源；另一方面，让 LLM 可以获得最新的信息，如今天的天气。

RAG 的实现存在两条路径：基于外部搜索引擎、基于向量数据库。前者依赖于已有的搜索引擎，优化空间优先，因此我们聚焦于后者。

RAG 核心只有三步：index、retrieve、generate。
- index：将知识分片，随后通过 embedding 模型生成向量，存入向量数据库
- retrieve：从向量数据库中取出与 query 最接近的 k 片知识
- generate：将知识插入 prompt 中，发送给 LLM 得到最终结果

以下是 LightRAG 对 RAG 过程的定义：

$$
    \mathcal{M} = \Big( \mathcal{G}, ~~ \mathcal{R}=(\varphi, \psi) \Big),~~~
    \mathcal{M}(q; \mathcal{D}) = \mathcal{G}\Big(q, \psi(q; \hat{\mathcal{D}})\Big),~~~\hat{\mathcal{D}} = \varphi(\mathcal{D})
$$

- $$\mathcal{M}:~$$RAG 框架
- $$\mathcal{G}:~$$generate 模块
- $$\mathcal{R}:~$$retrieve 模块
- $$\varphi:~$$data indexer
- $$\psi:~$$data retriever
- $$\mathcal{D}:~$$原始数据库
- $$\hat{\mathcal{D}}:~$$特殊数据结构

> 随着上下文窗口变大，有人质疑 RAG 的必要性。其实将信息放在上下文中相当于通过注意力机制来检索关键信息，这在压测下表现一般，尤其是问题需要依赖多个分散的信息推理而来。
{: .prompt-info }

## 优化

### indexing

- chunk 分片
- 对文档先总结再 embed
- raptor: 对文档做多层汇总，从而对多文档相关问题也能很好地解答
- LightRAG: 利用 LLM 根据文档建图，再为其中每个点和边创建索引

### retrieval

直接根据用户的 query 搜索效果显然不好，可以通过 LLM 来优化 query

- step-forward：拆分为复数个具体的 query。适用于复杂问题，将一个宽泛的问题分解为多个子问题，分别检索后综合答案。

    ```
    优化前：什么是机器学习？
    优化后：
    - 机器学习的定义是什么？
    - 机器学习有哪些主要类型？
    - 机器学习的应用场景有哪些？
    ```

- step-back：提炼为更抽象的 query。当原问题过于具体时，退一步思考更高层次的概念，帮助检索到更全面的背景知识。

    ```
    优化前：怎么做西红柿炒鸡蛋？
    优化后：家庭常见的烹饪方法有哪些？
    ```

- RAG Fusion：生成多条相似的 query，通过 RRF（Reciprocal Rank Fusion）算法综合分析查询的结果。多角度提问，提高召回率。

    ```
    优化前：如何提高模型准确率？
    优化后：
    - 如何提高模型准确率？
    - 提升模型性能的方法有哪些？
    - 模型优化的常见技巧
    - 改善预测精度的策略
    ```

- least-to-most：拆分为多个逐渐深入的问题，并且按顺序解决它们。从简单到复杂，前一个问题的答案作为后续问题的上下文。

    ```
    优化前：如何部署一个高可用的微服务架构？
    优化后：
    1. 微服务架构
    2. 微服务架构 - 微服务架构的核心组件
    3. 微服务架构 - 微服务架构的核心组件 - 微服务服务的高可用性
    4. 微服务架构 - 微服务架构的核心组件 - 微服务服务的高可用性 - 微服务部署的最佳实践
    ```

- HyDE：生成一篇虚假的文档，用它来搜索最近的知识。让 LLM 先生成一个假设的答案，用这个答案的向量去检索，往往比问题本身更接近真实文档。

    ```
    优化前：什么是 Transformer 的自注意力机制？
    优化后（生成假设答案）：
    Transformer 的自注意力机制是一种允许模型在处理序列时关注不同位置的方法。
    它通过 Query、Key、Value 三个矩阵计算注意力权重，使用点积和 softmax 
    来确定每个位置对当前位置的重要性。这种机制使模型能够捕捉长距离依赖关系...
    ```

### routing

选择不同路径
- logical routing: 让 LLM 选择要查询的数据库
- semantic routing: embed 之后选择要使用的 prompt（让 LLM 扮演不同的角色）

### query construction

数据库可以提供一些固定参数以供筛选，例如时间和长度。实现过程中可以将它们作为 function call 的参数来传入。

### 流程优化

- 为结果打分，决定是否要重新搜索（通过其他渠道或优化 query）
- retrive 较多文献，然后通过 LLM 精细筛选

## 参考

{% include embed/youtube.html id='sVcwVQRHIc8' %}
